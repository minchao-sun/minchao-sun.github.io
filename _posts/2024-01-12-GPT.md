---
title: "GPT"
classes: wide
toc: true
toc_sticky: true
tag:
  - NLP
  - LLM
categories:
  - Machine Learning
---

这篇文章总结GPT1论文中的一些要点。[1]

GPT是一个生成模型，需要基于前面的词预测后面的词，因此用了Transformer模型中的解码器架构。我们也把这类模型称为自回归模型。

## Objective function

### Unsupervised pre-training

给定一个未标号的文本U，每个词表示成$u_i$ ，我们标记为：$U=\{u_1,...,u_n\}$

GPT要最大化下面的似然函数：

$$
L_1(U)=\sum_i\log P(u_i|u_{i-k},...,u_{i-1};\Theta)
$$

对于每个词$u_i$，基于前面的k个词，估计第i个词是$u_i$的概率，再全部乘起来（因为有log，相加就等于概率相乘）。k被称为上下文窗口大小（context window size）。

### Supervised fine-tuning

微调步骤中的目标函数变成了

Maximize:

$$
L_2(C)=\sum_{(x,y)}\log P(y|x^1,...,x^m)
$$

实际上GPT还引入了前面预训练模型的目标函数，最后的目标函数是：

$$
L_3(C)=L_2(C)+\lambda * L_1(C)
$$

$\lambda$ 是一个超参数

## Reference

[1] Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
