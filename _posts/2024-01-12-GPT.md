---
title: "GPT"
tag:
  - NLP
  - LLM
categories:
  - Machine Learning
---

这篇文章总结GPT1论文中的一些要点。[1]

GPT是一个生成模型，需要基于前面的词预测后面的词，因此用了Transformer模型中的解码器架构。我们也把这类模型称为自回归模型。

## Objective function

### Unsupervised pre-training

给定一个未标号的文本U，每个词表示成$u_i$ ，我们标记为：$U=\{u_1,...,u_n\}$

GPT要最大化下面的似然函数：

$$
L_1(U)=\sum_i\log P(u_i|u_{i-k},...,u_{i-1};\Theta)
$$

对于每个词$u_i$，基于前面的k个词，估计第i个词是$u_i$的概率，再全部乘起来（因为有log，相加就等于概率相乘）。k被称为上下文窗口大小（context window size）。

## Reference

[1] Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
