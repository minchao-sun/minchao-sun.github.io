---
title: "å¦‚ä½•ä¼°è®¡Transformeræ¨¡å‹çš„å‚æ•°é‡çº§"
tag:
  - LLM
categories:
  - Machine Learning
---

è¿™ç¯‡æ–‡ç« æ—¨åœ¨å¸®åŠ©ä½ å¿«é€Ÿä¼°ç®—ä¸€ä¸ªä½¿ç”¨äº†Transformeræ¶æ„çš„å¤§æ¨¡å‹çš„å‚æ•°æ•°é‡ã€‚ï¼ˆä¸è®ºæ˜¯ä½¿ç”¨äº†å®Œæ•´çš„Transformerè¿˜æ˜¯åªä½¿ç”¨äº†ä¸€éƒ¨åˆ†ï¼‰

è®¡ç®—æ–¹æ³•æ¥è‡ªäºDmytro Nikolaiev (Dimid)çš„[åšå®¢](https://towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0)ã€‚

## å¤ªé•¿ä¸çœ‹ğŸ™ˆ

Too long don't read

![å›¾ç‰‡](https://segmentfault.com/img/remote/1460000043888826)

| å‚æ•°                 | ç²¾ç¡®å…¬å¼                                           | ä¼°è®¡å…¬å¼                        |
| -------------------- | -------------------------------------------------- | ------------------------------- |
| Multi-head attention | $4(d_{model}^2+d_{model})$                         | $4d_{model}^2$                  |
| Feed-forward         | $2d_{model}d_{ff}+d_{model}+d_{ff}$                | $2d_{model}d_{ff}$              |
| Layer norm           | $2 d_{model}$                                      | 0                               |
| Encoder              | $4d_{model}^2+2d_{model}d_{ff}+9d_{model}+d_{ff}$  | $4d_{model}^2+2d_{model}d_{ff}$ |
| Decoder              | $8d_{model}^2+2d_{model}d_{ff}+15d_{model}+d_{ff}$ | $8d_{model}^2+2d_{model}d_{ff}$ |

