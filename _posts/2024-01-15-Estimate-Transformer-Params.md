---
title: "å¦‚ä½•ä¼°è®¡Transformeræ¨¡å‹çš„å‚æ•°é‡çº§"
classes: wide
tag:
  - LLM
categories:
  - Machine Learning
---

è¿™ç¯‡æ–‡ç« æ—¨åœ¨å¸®åŠ©ä½ å¿«é€Ÿä¼°ç®—ä¸€ä¸ªä½¿ç”¨äº†Transformeræ¶æ„çš„å¤§æ¨¡å‹çš„å‚æ•°æ•°é‡ã€‚ï¼ˆä¸è®ºæ˜¯ä½¿ç”¨äº†å®Œæ•´çš„Transformerè¿˜æ˜¯åªä½¿ç”¨äº†ä¸€éƒ¨åˆ†ï¼‰

è®¡ç®—æ–¹æ³•æ¥è‡ªäºDmytro Nikolaiev (Dimid)çš„[åšå®¢](https://towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0)ã€‚

## å¤ªé•¿ä¸çœ‹ğŸ™ˆ

Too long don't read

| å‚æ•°                 | ç²¾ç¡®å…¬å¼                                           | ä¼°è®¡å…¬å¼                        | æ›´ç²—ç•¥çš„ä¼°è®¡                       |
| -------------------- | -------------------------------------------------- | ------------------------------- | ---------------------------------- |
| Multi-head attention | $4(d_{model}^2+d_{model})$                         | $4d_{model}^2$                  | /                                  |
| Feed-forward         | $2d_{model}d_{ff}+d_{model}+d_{ff}$                | $2d_{model}d_{ff}$              | /                                  |
| Layer norm           | $2 d_{model}$                                      | 0                               | /                                  |
| Encoder              | $4d_{model}^2+2d_{model}d_{ff}+9d_{model}+d_{ff}$  | $4d_{model}^2+2d_{model}d_{ff}$ | $12d_{model}\approx10 d_{model}^2$ |
| Decoder              | $8d_{model}^2+2d_{model}d_{ff}+15d_{model}+d_{ff}$ | $8d_{model}^2+2d_{model}d_{ff}$ | $16d_{model}\approx10 d_{model}^2$ |

  



